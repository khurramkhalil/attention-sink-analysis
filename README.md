# attention-sink-analysis
A Python framework for in-depth analysis and visualization of 'attention sink' phenomena in Transformer language models. Investigate how initial tokens act as contextual anchors, quantify their influence across layers/heads, and probe their representations. Built with PyTorch &amp; Hugging Face. Inspired by recent advancements in long-context LLMs.
