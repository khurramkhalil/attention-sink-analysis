ATTENTION SINK ANALYSIS - AGGREGATED SUMMARY REPORT
======================================================================
Generated on: 2025-06-21 13:54:44

DATASET OVERVIEW:
------------------------------
Total model-text combinations analyzed: 20
Unique models: 4
Unique text types: 5

Models analyzed:
  - gpt2: 5 text types
  - gpt2-large: 5 text types
  - gpt2-medium: 5 text types
  - microsoft/DialoGPT-medium: 5 text types

Text types analyzed:
  - code: 4 models
  - dialogue: 4 models
  - narrative: 4 models
  - short: 4 models
  - technical: 4 models

KEY FINDINGS - SINK POSITION ANALYSIS:
---------------------------------------------
Position 1 - Avg: 0.4620 (±0.0813)
Position 2 - Avg: 0.0102 (±0.0084)
Position 3 - Avg: 0.0088 (±0.0095)
Position 4 - Avg: 0.0136 (±0.0134)

Strongest sink position: Position 1 (0.4620)

KEY FINDINGS - LAYER ANALYSIS:
-----------------------------------
Average peak layer for sink attention: 19.9
Average peak attention score: 0.7310
Average attention across all layers: 0.4945

KEY FINDINGS - REPRESENTATION SIMILARITY:
---------------------------------------------
Average sink representation similarity: 0.5052
Average maximum similarity: 0.8215
Average minimum similarity: 0.2568

FILES GENERATED:
--------------------
1. attention_by_sink_position.csv
   - Attention received by each sink position (1-4)
   - Rows: Model-text combinations
   - Columns: Sink positions + metadata

2. sink_representation_similarity_by_layer.csv
   - Cosine similarity between sink representations
   - Rows: Model-text combinations
   - Columns: Layer-wise similarity scores

3. attention_to_sinks_by_layer.csv
   - Attention flow to sink tokens by layer
   - Rows: Model-text combinations
   - Columns: Layer-wise attention scores
